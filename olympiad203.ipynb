{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## data wrangling tools\nimport pandas as pd\npd.options.mode.chained_assignment = None \nimport numpy as np\nimport os\n\n#stats for ensembling\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n#Preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\n\n#Ensembling\nfrom scipy import stats as st\n\n#visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Dimensionality reduction\nfrom sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport pandas as pd\n\nclass ReduceDim:\n    def __init__(self, n):\n        self.n = n\n        self.pca = PCA(n_components=n)\n\n    def fit_transform(self, M):\n        M_ = self.pca.fit_transform(M)\n        M = pd.concat([M, pd.DataFrame(M_, columns=[f'PCA{i+1}' for i in range(self.n)])], axis=1)\n        return M\n\n    def transform(self, N):\n        N_ = self.pca.transform(N)\n        N = pd.concat([N, pd.DataFrame(N_, columns=[f'PCA{i+1}' for i in range(self.n)])], axis=1)\n        return N","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_outlier(df):\n    for col in ['Verifications_','record_number']:\n        Q1 = np.percentile(df[col], 25, method='midpoint')\n        Q3 = np.percentile(df[col], 75, method='midpoint')\n        IQR = Q3-Q1\n        upper = Q3+1.5*IQR\n        lower = Q1-1.5*IQR\n        median = df[col].median()\n        df[col] = np.where((df[col] > upper) | (df[col] < lower), median, df[col])\n        return df  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Category encoder\ndef cat_encoder(X_train, X_test, cat_cols):\n    encoder = OrdinalEncoder()\n    train_encoder = encoder.fit_transform(X_train[cat_cols]).astype(int)\n    test_encoder = encoder.transform(X_test[cat_cols]).astype(int)\n    for col in cat_cols:\n        X_train[col] = train_encoder[:, cat_cols.index(col)]\n        X_test[col] = test_encoder[:, cat_cols.index(col)]\n    encoder_cols = cat_cols\n    return X_train, X_test, encoder_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import TransformerMixin,BaseEstimator\nfrom sklearn.feature_selection import VarianceThreshold\n\n#Implementingthe Transformer class\nclass low_var(TransformerMixin):\n    def __init__(self,threshold=0.3):\n        self.threshold=threshold\n    def fit(self,X,y=None):\n        col_vars=X.var()\n        self.col_to_drop=col_vars[col_vars<self.threshold].index\n        return self\n    def transform(self,X):\n        assert self.col_to_drop is not None, 'Drop_col error, must be fitted before predict'\n        X.drop(self.col_to_drop, axis=1, inplace=True)\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the dataset\nFILEPATH=\"/kaggle/input/analytic-olympiad\"\ndf_train=pd.read_csv(os.path.join(FILEPATH,'train.csv'))\ndf_test=pd.read_csv(os.path.join(FILEPATH,'test.csv'))\ndf_train=df_train.fillna(0)\ndf_test=df_test.fillna(0)\n\n\n#Defining the features\ntarget_col01='primary_close_flag'\ntarget_col02='final_close_flag'\ncat_cols=df_train.select_dtypes(include=\"object\").columns.to_list()\nnum_cols=df_train.select_dtypes(include=[\"int64\",\"float64\"]).columns.to_list()[:-2]\n\n#Definging the training and testing dataset\nX = df_train.drop([f'{target_col01}',f'{target_col02}'],axis=1).reset_index(drop=True)\ny = df_train[f'{target_col02}'].reset_index(drop=True)\nX_=df_test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop_col\nencoded_features = ['encoded_payment_' + str(i) for i in range(25)]\ndrop_cols = ['customer_id', 'firstname', 'lastname']\nX.drop(drop_cols, axis=1, inplace=True)\nX_.drop(drop_cols, axis=1, inplace=True)\n\n#Reassigning cat_cols\ncat_cols=X.select_dtypes(include=\"object\").columns.to_list()\nnum_cols=X.select_dtypes(include=[\"int64\",\"float64\"]).columns.to_list()[:-2]\n\n#Adding more column Train\nX['Due_']=X['final_term']-X['primary_term']\nX['Delay_']=X['days_till_final_close']-X['days_till_primary_close']\nX['Verifications_delay']=X['days_till_final_close']-X['final_term']\nX['Verifications_time']=X['days_till_primary_close']-X['primary_term']\nX['Verifications_'] = X['days_till_primary_close'] * X['primary_term']\nX['days_up_primary']=X['primary_term']-X['days_till_primary_close']\nX['days_up_final']=X['final_term']-X['days_till_final_close']\n\n#Adding more column Test\nX_['Due_']=X_['final_term']-X_['primary_term']\nX_['Delay_']=X_['days_till_final_close']-X_['days_till_primary_close']\nX_['Verifications_delay']=X_['days_till_final_close']-X_['final_term']\nX_['Verifications_time']=X_['days_till_primary_close']-X_['primary_term']\nX_['Verifications_']=X_['days_till_primary_close']*X_['primary_term']\nX_['days_up_primary']=X_['primary_term']-X_['days_till_primary_close']\nX_['days_up_final']=X_['final_term']-X_['days_till_final_close']\n\n# for df in [X,X_]:\n#     try:\n#         df['Verifications_ratio'] = df['days_till_primary_close'] // df['primary_term']\n#     except ZeroDivisionError:\n#         df['Verifications_ratio'] = 0\n    \n#Removing columns with low variance\n# category_encoders\nX, X_, cat_cols = cat_encoder(X, X_, cat_cols)\n#Removing outliers\nX=remove_outlier(X)\nX_=remove_outlier(X_)\n\n#dimensionality reduction\ndim=ReduceDim(2)\nX=dim.fit_transform(X)\nX_=dim.transform(X_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualise dimensionality reduction\nM=pd.concat([X,y],axis=1)\nsns.scatterplot(data=M, x=\"PCA1\", y=\"PCA2\", hue=f\"{target_col02}\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualising numerical columns\nfig,axs=plt.subplots(2, 2, figsize=(10, 2*5))\nax=axs.flatten()\nfor i,col in enumerate(['record_number','Verifications_','primary_term','days_till_primary_close']):\n    sns.histplot(x=X[col],kde=True,ax=ax[i])\nplt.tight_layout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalization of data.\nscale=preprocessing.StandardScaler()\nX=pd.DataFrame(scale.fit_transform(X),index=X.index,columns=X.columns)\nX_=pd.DataFrame(scale.transform(X_),index=X_.index,columns=X_.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################### XGB CLASSSIFIER #######################\nfrom xgboost import XGBClassifier\ndef xgb_model():\n    xgb_01={\n         'n_estimators': 120,\n         'learning_rate': 0.5619556985982561,\n         'max_depth': 136,\n         'min_child_weight': 10,\n         'reg_alpha': 0.4632934146772244,\n         'reg_lambda': 13,\n         'gamma': 0.7471461478419492,\n         'colsample_bytree': 0.8939543526804394,\n         'colsample_bylevel': 0.33219308427147426 \n    }\n    xgb_02={\n         'n_estimators': 320,\n         'learning_rate': 0.6771864073202802,\n         'max_depth': 179,\n         'min_child_weight': 14,\n         'reg_alpha': 0.6456816599087696,\n         'reg_lambda': 10,\n         'gamma': 1.264355703745565,\n         'colsample_bytree': 0.6432920257822892,\n         'colsample_bylevel': 0.4547371625752076\n    }\n    \n   # return XGBClassifier(**xgb_01)\n    return XGBClassifier(**xgb_02)\n################## LGBM CLASSIFIER ############################\nfrom lightgbm import LGBMClassifier\ndef lgbm_model():\n    lgbm_01={\n         'n_estimators': 229,\n         'learning_rate': 0.5185147161031304,\n         'max_depth': 70,\n         'min_child_weight': 10,\n         'reg_alpha': 0.699472780990506,\n         'reg_lambda': 7,\n         'colsample_bytree': 0.3072359964466818\n    }\n    lgbm_02={\n         'n_estimators': 327,\n         'learning_rate': 0.7862270379341548,\n         'max_depth': 199,\n         'min_child_weight': 13,\n         'reg_alpha': 0.8800992852019061,\n         'reg_lambda': 6,\n         'colsample_bytree': 0.8218273150299635\n    }\n    return LGBMClassifier(**lgbm_02)\n#     return LGBMClassifier(**lgbm_01)\n    \n####################### CATBOOST CLASSIFIER ###############\nfrom catboost import CatBoostClassifier \ndef cat_model():\n    cat_01={}\n    #return CatBoostClassifier(**cat_01)\n    return CatBoostClassifier(**cat_01)\n\n############## TPOT Classifier ###############\nfrom tpot import TPOTClassifier\ndef tpot_model():\n    tpot_01={\n        'generations':2, \n        'population_size':15,\n        'scoring':'accuracy'\n    }\n    return TPOTClassifier(**tpot_01)\n\n############## RANDOM FOREST ########\nfrom sklearn.ensemble import RandomForestClassifier\ndef forest_model():\n    return RandomForestClassifier()\n\n############## HIST CLASSIFIER\nfrom sklearn.ensemble import HistGradientBoostingClassifier\ndef hist_model():\n    return HistGradientBoostingClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import regularizers\nimport tensorflow as tf\n\nfrom tensorflow import keras\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=15,\n    monitor='val_binary_crossentropy',\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\n#It is better to reduce the learning rate as we do training.\nlr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n  0.001,\n  decay_steps=X.shape[0]*1,\n  decay_rate=1,\n  staircase=False)\n\ndef tf_model():\n    model_tf=tf.keras.Sequential([\n        keras.layers.Input(shape=[57,]),\n        keras.layers.Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.003)),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.003),),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001),),\n        keras.layers.Dense(8, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(2, activation='sigmoid')])\n        \n    model_tf.compile(optimizer= keras.optimizers.Adam(lr_schedule),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    return model_tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nmodel = lgbm_model()\nmodel.fit(X, y)\nperm = PermutationImportance(model, random_state=1).fit(X.head(1500), y.head(1500))\neli5.show_weights(perm, feature_names = X.columns.tolist())\n#primary_term,days_till_primary_close ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance=eli5.explain_weights_df(perm, feature_names=X.columns.tolist())\ndrop_col = feature_importance[feature_importance['weight'] == 0]\ndrop_col = drop_col['feature'].tolist()\n# for col in drop_col:\n#     X.drop([col], axis=1, inplace=True)\n#     X_.drop([col],axis =1, inplace =True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_split=10\nrandom_state=42\npreds=pd.DataFrame()\nkf = StratifiedKFold(n_splits=n_split, random_state=random_state, shuffle=True)\nfor i,(train_index, val_index) in enumerate(kf.split(X,y)):\n    models={}\n    class_probs={}\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    \n    ###########Deep Neural_MODEL###################\n    y_train, y_val = [tf.keras.utils.to_categorical(y.iloc[index]) for index in [train_index, val_index]]\n    model=tf_model()\n   # model.fit(X_train,y_train,validation_data=[X_val,y_val],epochs=1,\n                    # callbacks=[early_stopping],batch_size=200)\n   # models['tf_model']=model\n    \n    ######Non Neural Neural Networks#######################\n    y_train, y_val = y.iloc[train_index],y.iloc[val_index]\n    ###XGB classifier ########\n    model=xgb_model()\n    model.fit(X_train, y_train,eval_set=[(X_val, y_val)])\n    models['xgb_model']=model\n    ####LGBM classifier########\n    model=lgbm_model()\n    model.fit(X_train, y_train,eval_set=[(X_val, y_val)])\n    models['lgbm_model']=model\n    ####LGBM classifier01########\n    model=lgbm_model()\n    model.fit(X_train, y_train,eval_set=[(X_val, y_val)])\n    models['lgbm_model01']=model\n    ####CAT classifier########\n    model=cat_model()\n    model.fit(X_train, y_train,eval_set=[(X_val, y_val)])\n    models['cat_model']=model\n    ####CAT classifier01########\n    model=cat_model()\n    model.fit(X_train, y_train,eval_set=[(X_val, y_val)])\n    models['cat_model01']=model\n    ####### RANDOM FOREST ####\n    model=forest_model()\n    model.fit(X_train,y_train)\n    models['forest_model']=model\n    ######### HIST GRAD#######\n    model=hist_model()\n    model.fit(X_train,y_train)\n    models['hist_model']=model\n    \n    \n    \n    #####Ensemble Models#######################\n    for model_name, model in models.items():\n        if model_name =='tf_model':\n             probs = model.predict(X_)\n        else :\n            probs=model.predict_proba(X_)\n        class_probs[model_name] = probs\n    ensemble_probs = np.mean(list(class_probs.values()), axis=0)\n    probs= np.argmax(ensemble_probs, axis=1)\n    preds.insert(loc=0, column=f'fold_{i+1}', value=probs)\n    \n    print(f'############## FOLD{i+1}########################')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats as st\npreds['mode']=preds.apply(lambda x:st.mode(x)[0],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axs=plt.subplots(2,2,figsize=(12, 5*2))\nax=axs.flatten()\nfor i,model in enumerate(['xgb_model','lgbm_model','cat_model','forest_model']):\n    feat_imp = pd.Series(models[model].feature_importances_, index=X.columns)\n    feat_imp.nlargest(10).plot(kind='barh',ax=ax[i])\n    plt.xticks(rotation=45)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result=pd.read_csv('/kaggle/input/final/submission_final.csv')\nresult[f'{target_col02}']=preds['mode']\nresult.to_csv('submission_final.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}